# Repository Info

This repository facilitates my research by enforcing modularity and housing various Transformer implementations.  

Experiments are conducted in their own, separate repositories.
<br></br>
# Current implementations

## Vanilla
This implementation follows the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.